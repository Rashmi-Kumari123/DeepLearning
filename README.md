# 3-Layer Neural Network Approximation

This project demonstrates a **simple 3-layer neural network** (1 neuron per layer) implemented from scratch using **NumPy**. The network is trained to approximate a non-linear function using **Mean Squared Error (MSE)** loss.  

## Features
- Custom implementation of **forward pass**, **backward pass**, and **gradient descent** updates.
- Supports **Tanh** and **ReLU** activation functions.
- Standardizes input data for better convergence.
- Visualizes training/validation loss and compares predicted output vs true function.

## Usage
1. Run the notebook cells in order.
2. Set the activation function: `'tanh'` or `'relu'`.
3. Observe the training and test loss plots along with NN predictions.

## Dependencies
- Python 3.x
- NumPy
- Matplotlib
